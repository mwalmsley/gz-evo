# encoder_hub_path: "local:/home/walml/Dropbox (The University of Manchester)/gz-evo/baselines/finetune/jiruf12f/checkpoints/12.ckpt"
encoder_hub_path: "local:/share/nas2/walml/gz-evo/results/downstream/dnb_debug/jiruf12f/checkpoints/12.ckpt"
architecture_name: convnext_nano
from_scratch: False
channels: 3  # used for augs
input_size: 224  # used for augs
normalize: False

batch_size: 32
patience: 6
stochastic_depth: 0.0



# deep finetune with decay
# learning_rate: 0.00001  # 1e-5
# lr_decay: 0.25
# weight_decay: 0.1
# dropout_prob: 0.5
# n_blocks: 5

# or linear only

learning_rate: 0.0001  # 1e-4
lr_decay: 0.25  # no effect
weight_decay: 0.1
dropout_prob: 0.5
n_blocks: 0


# parameters below here don't change / do much

max_epochs: 5000  # rely on early stopping. 200 epochs is plenty for n_blocks=5, but not enough for n_blocks=0 on very small datasets e.g. is-lsb with divisor>=2 
always_train_batchnorm: False
check_val_every_n_epoch: 1
# only does something if cosine_schedule is True
cosine_schedule: False
warmup_epochs: 0
max_cosine_epochs: 100
max_learning_rate_reduction_factor: 0.01