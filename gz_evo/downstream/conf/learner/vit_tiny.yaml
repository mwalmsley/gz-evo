
# encoder_hub_path: hf_hub:timm/vit_tiny_patch16_224.augreg_in21k  # smallest default ViT model in timm, not astro pretrained
# pretrained: False  # only has an effect for timm models
# from_scratch: True
# pretrained: True  # only has an effect for timm models
# from_scratch: False
# learning_rate: 0.0001  # 1e-4
# lr_decay: 0.5  # moderate LR decay, unlike for vitso-400m
# weight_decay: 0.1
# dropout_prob: 0.5

# encoder_hub_path: hf_hub:mwalmsley/wip-hybrid-encoder-equ5ifvr  # mae pretrained version, on gz evo tiny
encoder_hub_path: hf_hub:mwalmsley/wip-hybrid-encoder-n0jvg4dc  # mae pretrained version, on strong lensing
pretrained: True 
from_scratch: False
learning_rate: 0.0001  
lr_decay: 0.8
weight_decay: 0.0
dropout_prob: 0.5

architecture_name: vit_tiny_patch16_224


channels: 3  # used for augs
input_size: 224  # used for augs
normalize: False

batch_size: 32



patience: 10
stochastic_depth: 0.0

# deep finetune with decay

n_blocks: -1


# parameters below here don't change / do much

max_epochs: 5000  # rely on early stopping. 200 epochs is plenty for n_blocks=5, but not enough for n_blocks=0 on very small datasets e.g. is-lsb with divisor>=2 
always_train_batchnorm: False
check_val_every_n_epoch: 1
# only does something if cosine_schedule is True
cosine_schedule: False
warmup_epochs: 0
max_cosine_epochs: 100
max_learning_rate_reduction_factor: 0.01